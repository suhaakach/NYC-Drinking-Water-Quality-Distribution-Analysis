{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: 2024 NYC - Drinking Water Quality Distribution\n",
        "subtitle: STAT 4188\n",
        "author: Isabelle Perez & Suha Akach\n",
        "format:\n",
        "  revealjs:\n",
        "    slide-number: true\n",
        "    preview-links: true\n",
        "    theme: serif\n",
        "    transition: slide\n",
        "execute:\n",
        "  cache: true\n",
        "echo: false\n",
        "bibliography: ref.bib\n",
        "---"
      ],
      "id": "503f30fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Objectives\n",
        "::: {.incremental}\n",
        "- Monitering water quality by different chemical presence over time.\n",
        "- Finding areas of concern for water quality in NYC.\n",
        "- Finding predictors of water quality.\n",
        ":::\n",
        "\n",
        "# Data Collection\n",
        "::: {.incremental}\n",
        "- “Drinking Water Quality Distribution Monitoring Data” provided by the Department of Environmental Protection (DEP) [@nycopendata2024]. \n",
        "- Data from: 09/01/2024 till 09/30/2024.\n",
        "- Tracked variables: Chlorine, Turbidity, Coliform, E.coli.\n",
        ":::\n",
        "\n",
        "# Data Cleaning\n",
        "## {.scrollable}\n",
        "- Dropped and cleaned columns using Pandas.\n",
        "    - Unessasary columns: Sample Number, Fluoride (mg/L), Sample Time.\n",
        "- Improved column readibilty.\n",
        "- Strandized format of our sample time column.\n",
        "- Fixed column data types."
      ],
      "id": "81686bd3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
        "                                                            r2_score)\n",
        "from uszipcode import SearchEngine\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data = pd.read_csv('data/Drinking_Water_Quality_Distribution_Monitoring_Data_20241116.csv',\n",
        "                                                                  low_memory = False)                       \n",
        "\n",
        "# rename columns\n",
        "data.rename(columns = {'Sample Date': 'sample_date', 'Sample Site': 'sample_site', \n",
        "              'Sample class': 'sample_class', 'Residual Free Chlorine (mg/L)': 'chlorine',\n",
        "              'Turbidity (NTU)': 'turbidity', 'Coliform (Quanti-Tray) (MPN /100mL)': 'coliform',\n",
        "                                      'E.coli(Quanti-Tray) (MPN/100mL)': 'ecoli'}, inplace = True)"
      ],
      "id": "f644cb2f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "# ensure sample time column is clean\n",
        "data.dropna(subset = ['Sample Time'], inplace = True)\n",
        "\n",
        "# standardize format of sample time\n",
        "data['Sample Time'] = data['Sample Time'].apply(lambda x: x[11:16] \n",
        "    if len(x) > 5 else x)\n",
        "\n",
        "# change to datetime format\n",
        "data['sample_date'] = pd.to_datetime(data['sample_date'] + ' ' + \n",
        "    data['Sample Time'])\n",
        "\n",
        "# change turbidity to float\n",
        "data.loc[data['turbidity'] == '<0.10', 'turbidity'] = '0.10'\n",
        "data['turbidity'] = data['turbidity'].apply(lambda x: float(x))"
      ],
      "id": "935f43fd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Feature Engineering\n",
        "## {.scrollable}\n",
        "- Merged sample site information with our water quality dataset using USZIPCODE package.\n",
        "    - This helps us track different chemical values depending on borough, median household income, median_home_value, etc.\n"
      ],
      "id": "9b750571"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "# read in data for sampling sites\n",
        "sites = pd.read_csv('data/sampling_sites_extended.csv')\n",
        "\n",
        "# use search engine to find demographic data\n",
        "search = SearchEngine() \n",
        "\n",
        "zipdata = [] \n",
        "\n",
        "for zipcode in sites['ZIP Code']: \n",
        "  info = search.by_zipcode(zipcode)\n",
        "\n",
        "  if bool(info) == True: \n",
        "    zipdata.append({\n",
        "      'ZIP Code': zipcode,\n",
        "      'housing_units': info.housing_units / 1000,\n",
        "      'occupied_housing_units': info.occupied_housing_units / 1000, \n",
        "      'median_home_value': info.median_home_value / 1000,\n",
        "      'median_household_income': info.median_household_income / 1000\n",
        "    })\n",
        "\n",
        "# add demographic data to sampling sites\n",
        "zipdata = pd.DataFrame(zipdata) \n",
        "sites = pd.merge(sites, zipdata, how = 'inner', on = 'ZIP Code')\n",
        "sites = sites.drop_duplicates()\n",
        "\n",
        "# merge with location based information\n",
        "sites.rename(columns = {'Sample Site': 'sample_site'}, inplace = True)\n",
        "\n",
        "data = pd.merge(data, sites, on = 'sample_site')"
      ],
      "id": "f921c299",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Created a 'complaint' target variable that indicates safe water quality based on national water quality thresholds. "
      ],
      "id": "1de2586b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "def preprocess_data(df):\n",
        "    df['compliant'] = (\n",
        "        (df['chlorine'] > 0.2) & (df['chlorine'] < 1.0) & \n",
        "        (df['ecoli'] <= 0) & \n",
        "        (df['turbidity'] < 0.3) & \n",
        "        (df['coliform'] < 2.67)\n",
        "    ).astype(int)\n",
        "    return df"
      ],
      "id": "bdc572d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#  Visualization of Chemical Levels by the Least and Most Densely Populated Boroughs\n",
        "## {.scrollable}\n",
        "### Staten Island vs Manhattan over time"
      ],
      "id": "3a50a4f0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "merged_df = pd.read_csv('data/borough.csv')\n",
        "\n",
        "# list of boroughs to plot (only Manhattan and Staten Island)\n",
        "boroughs_to_plot = ['Staten Island', 'Manhattan']\n",
        "\n",
        "# figure to hold 4 subplots (one for each variable per borough)\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))\n",
        "\n",
        "# threshold values for each chemical\n",
        "thresholds = {\n",
        "    'chlorine': 4,  # mg/L\n",
        "    'turbidity': 0.3,  # NTU\n",
        "    'coliform': 2.67,  # MPN/100mL\n",
        "    'ecoli': 0  # MPN/100mL\n",
        "}\n",
        "\n",
        "# loop over each borough and create plots\n",
        "for i, borough in enumerate(boroughs_to_plot):\n",
        "    # filter data for the current borough\n",
        "    borough_data = merged_df[merged_df['borough'] == borough]\n",
        "\n",
        "    # plot for Coliform levels (MPN/100mL)\n",
        "    ax = axes[0, i]  # Top row\n",
        "    ax.plot(borough_data.index, borough_data['coliform'], label='Coliform (MPN/100mL)', color='green')\n",
        "    ax.axhline(y=thresholds['coliform'], color='green', linestyle='--', label='Max Coliform Threshold (2.67 MPN/100mL)')\n",
        "    ax.set_title(f'Coliform Levels for {borough} (MPN/100mL)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for E. coli levels (MPN/100mL)\n",
        "    ax = axes[1, i]  # Second row\n",
        "    ax.plot(borough_data.index, borough_data['ecoli'], label='E. coli (MPN/100mL)', color='red')\n",
        "    ax.axhline(y=thresholds['ecoli'], color='red', linestyle='--', label='Max E. coli Threshold (126 MPN/100mL)')\n",
        "    ax.set_title(f'E. coli Levels for {borough} (MPN/100mL)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for Turbidity levels (NTU)\n",
        "    ax = axes[2, i]  # Third row\n",
        "    ax.plot(borough_data.index, borough_data['turbidity'], label='Turbidity (NTU)', color='orange')\n",
        "    ax.axhline(y=thresholds['turbidity'], color='orange', linestyle='--', label='Max Turbidity Threshold (0.3 NTU)')\n",
        "    ax.set_title(f'Turbidity Levels for {borough} (NTU)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for Chlorine levels (mg/L)\n",
        "    ax = axes[3, i]  # Fourth row\n",
        "    ax.plot(borough_data.index, borough_data['chlorine'], label='Chlorine (mg/L)', color='blue')\n",
        "    ax.axhline(y=thresholds['chlorine'], color='blue', linestyle='--', label='Max Chlorine Threshold (4 mg/L)')\n",
        "    ax.set_title(f'Chlorine Levels for {borough} (mg/L)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "8e0b90d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "# Finding if Cetain Boroughs Predict Water Quality w/ Logistic Regression.\n",
        "\n",
        "## {.scrollable}\n",
        "### Function to preprocess and create the 'compliant' target variable."
      ],
      "id": "18308e7c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df['compliant'] = (\n",
        "        (df['chlorine'] > 0.2) & (df['chlorine'] < 1.0) & \n",
        "        (df['ecoli'] <= 0) & \n",
        "        (df['turbidity'] < 0.3) & \n",
        "        (df['coliform'] < 2.67)\n",
        "    ).astype(int)\n",
        "    return df"
      ],
      "id": "4e73ab53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Function to resample the data using SMOTE\n",
        "- This addresses the class imbalance by generating synthetic samples for the minority class (non-compliant), making the dataset more balanced."
      ],
      "id": "8aec7f1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "def resample_data(X, y):\n",
        "    smote = SMOTE(random_state=0)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled"
      ],
      "id": "78770d04",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter tuning with GridSearchCV\n",
        "## {.scrollable}\n",
        "- **Lasso Regularization (L1)** shrinks some coefficients exactly to zero, effectively eliminating features. It works well for feature selection when only a subset of features are important.\n",
        "\n",
        "- **Ridge Regularization (L2)** shrinks coefficients towards zero but doesn't eliminate them. It works well when all features are expected to contribute to the prediction."
      ],
      "id": "46861cb1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "def grid_search_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10], # regularization strength\n",
        "        'penalty': ['elasticnet'], # combination of both L1 (Lasso) and L2 (Ridge).\n",
        "        'l1_ratio': [0.1, 0.5, 0.9],  # ratio between L1 and L2 (0 = L2, 1 = L1)\n",
        "        'solver': ['saga']\n",
        "    }\n",
        "\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', n_jobs=-1) # evaluation metric\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score (F1): {grid_search.best_score_}\")\n",
        "\n",
        "    return grid_search.best_estimator_"
      ],
      "id": "3561410b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function to Train and Evaluate the Model Using the Best Parameters from GridSearchCV\n",
        "## {.scrollable}\n",
        "- A 0.3 threshold to find predictions with probabilities above 0.3 which will be classified as 1 (compliant), and below will be 0 (non-compliant).\n",
        "- Standardized the features by removing the mean and scaling to unit variance. This is important for our model since it is sensitive to the scale of the features."
      ],
      "id": "721e1523"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, best_logreg, threshold=0.3):\n",
        "    # standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # fit the model with the best parameters\n",
        "    best_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # get predicted probabilities and adjust threshold\n",
        "    y_pred_prob = best_logreg.predict_proba(X_test_scaled)[:, 1]\n",
        "    y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
        "\n",
        "    # evaluation metrics\n",
        "    print('Accuracy:', accuracy_score(y_test, y_pred_adjusted))\n",
        "    print('Precision:', precision_score(y_test, y_pred_adjusted))\n",
        "    print('Recall:', recall_score(y_test, y_pred_adjusted))\n",
        "    print('F1 Score:', f1_score(y_test, y_pred_adjusted))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_adjusted)\n",
        "    print(cm)\n",
        "\n",
        "    return best_logreg"
      ],
      "id": "efbe8d85",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross Validation"
      ],
      "id": "c63f3e69"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo : true\n",
        "def plot_learning_curve(model, X_train, y_train):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        model, X_train, y_train, cv=5, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1')\n",
        "\n",
        "    # calculate the mean and standard deviation for plotting\n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    train_std = train_scores.std(axis=1)\n",
        "    test_mean = test_scores.mean(axis=1)\n",
        "    test_std = test_scores.std(axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, color='blue', label='Training score')\n",
        "    plt.plot(train_sizes, test_mean, color='red', label='Cross-validation score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
        "    plt.title(\"Learning Curve (Logistic Regression)\")\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()"
      ],
      "id": "68cd0c40",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results After Cross Validation\n",
        "## {.scrollable}"
      ],
      "id": "c934b71d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_feature_importance(model, X):\n",
        "    coefficients = model.coef_[0]\n",
        "    feature_names = X.columns\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefficients,\n",
        "        'Absolute Coefficient': abs(coefficients)\n",
        "    })\n",
        "    \n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "    # feature importance\n",
        "    print(feature_importance_df)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Absolute Coefficient'], color='skyblue')\n",
        "    plt.xlabel('Absolute Coefficient')\n",
        "    plt.title('Feature Importance (Logistic Regression)')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top\n",
        "    plt.show()"
      ],
      "id": "20ecbb3f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# step 1: preprocess data\n",
        "merged_df = preprocess_data(merged_df)\n",
        "\n",
        "# step 2: get dummy variables\n",
        "model_data = pd.get_dummies(merged_df, dtype=int)\n",
        "model_data.drop(columns=['zip code'], inplace=True)\n",
        "\n",
        "X = model_data.drop(columns=['compliant'])\n",
        "y = model_data['compliant']\n",
        "\n",
        "# step 3: resample data\n",
        "X_resampled, y_resampled = resample_data(X, y)\n",
        "\n",
        "# step 4: split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=4188)\n",
        "\n",
        "# step 5: perform gridsearchcv to find the best hyperparameters\n",
        "best_logreg = grid_search_tuning(X_train, y_train)\n",
        "\n",
        "# step 6: train and evaluate the logistic regression model with the best hyperparameters\n",
        "best_logreg = train_and_evaluate_model(X_train, y_train, X_test, y_test, best_logreg)\n",
        "\n",
        "# step 7: plot the learning curve\n",
        "plot_learning_curve(best_logreg, X_train, y_train)\n",
        "\n",
        "# step 8: plot feature importance\n",
        "plot_feature_importance(best_logreg, X)"
      ],
      "id": "c3e458ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis & Conclusion\n",
        "::: {.incremental}\n",
        "## {.scrollable}\n",
        "- The model has a accuracy of 75% and very high recall (99.7%), meaning it is excellent at detecting compliance. However, this comes at the cost of a lower precision (66.6%), which means the model over-predicts compliance, classifying too many non-compliant instances as compliant.\n",
        "- The F1 score is good (80.8%), reflecting a balance between precision and recall.\n",
        "- The model’s performance metrics indicate it is particularly good at identifying the positive class (compliant), but it could be improved in reducing false positives.\n",
        "- Feature importance indicates that boroughs along with household income, value and occupied units do have a positive indication of water compliance even though they aren't strong. \n",
        ":::\n",
        "\n",
        "# Thank You!\n",
        "\n",
        "# References\n",
        "::: {#refs}\n",
        "\n",
        ":::"
      ],
      "id": "d1618b10"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}