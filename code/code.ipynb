{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Project\"\n",
        "author: \"Suha Akach & Isabelle Perez\"\n",
        "toc: true\n",
        "number-sections: true\n",
        "highlight-style: pygments\n",
        "format: \n",
        "  html: \n",
        "    code-fold: true\n",
        "    html-math-method: katex\n",
        "  pdf: \n",
        "    geometry: \n",
        "      - top=30mm\n",
        "      - left=20mm\n",
        "  docx: default\n",
        "---"
      ],
      "id": "87e5b41c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import Data & Necessary Packages\n"
      ],
      "id": "c1ee42fa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
        "                                                            r2_score)\n",
        "from uszipcode import SearchEngine\n",
        "import seaborn as sns\n",
        "from pysal.lib import weights\n",
        "from pysal.model import spreg\n",
        "import gmplot\n",
        "import warnings\n",
        "import contextily as cx\n",
        "from shapely.geometry import Point\n",
        "import seaborn as sns\n",
        "import geopandas as gpd\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "053d1e67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv('../data/Drinking_Water_Quality_Distribution_Monitoring_Data_20241116.csv',\n",
        "                                                                  low_memory = False)                       "
      ],
      "id": "12f49b45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning \n"
      ],
      "id": "ef0caf50"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# rename columns\n",
        "data.rename(columns = {'Sample Date': 'sample_date', 'Sample Site': 'sample_site', \n",
        "              'Sample class': 'sample_class', 'Residual Free Chlorine (mg/L)': 'chlorine',\n",
        "              'Turbidity (NTU)': 'turbidity', 'Coliform (Quanti-Tray) (MPN /100mL)': 'coliform',\n",
        "                                      'E.coli(Quanti-Tray) (MPN/100mL)': 'ecoli'}, inplace = True)"
      ],
      "id": "ceb32435",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ensure sample time column is clean\n",
        "data.dropna(subset = ['Sample Time'], inplace = True)\n",
        "\n",
        "# standardize format of sample time\n",
        "data['Sample Time'] = data['Sample Time'].apply(lambda x: x[11:16] if len(x) > 5 else x)\n",
        "\n",
        "# change to datetime format\n",
        "data['sample_date'] = pd.to_datetime(data['sample_date'] + ' ' + data['Sample Time'])\n",
        "\n",
        "# change turbidity to float\n",
        "data.loc[data['turbidity'] == '<0.10', 'turbidity'] = '0.10'\n",
        "data['turbidity'] = data['turbidity'].apply(lambda x: float(x))\n",
        "\n",
        "# Check the first few rows after creating 'sample_date'\n",
        "print(data[['sample_date']].head())"
      ],
      "id": "d71daea8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# drop unecessary columns and rows\n",
        "data.drop(columns = ['Sample Number', 'Fluoride (mg/L)', 'Sample Time'], inplace = True)"
      ],
      "id": "661143e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# impute non-float values\n",
        "catval = {'<0.10': float(0.09), '<1': float(0.99), '>200.5': float(200.6)}\n",
        "data = data.replace(catval)\n",
        "\n",
        "# change data types to float\n",
        "data['chlorine'] = data['chlorine'].astype(float)\n",
        "data['coliform'] = data['coliform'].astype(float)\n",
        "data['ecoli'] = data['ecoli'].astype(float)"
      ],
      "id": "12125e5a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# check to make sure all values are possible\n",
        "data = data[data['chlorine'] > 0]\n",
        "data = data[data['coliform'] > 0]\n",
        "data = data[data['ecoli'] > 0]"
      ],
      "id": "b022d1b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# only 4 missing values total - drop\n",
        "data = data.dropna()\n",
        "\n",
        "data.shape"
      ],
      "id": "dfc88b6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in data for sampling sites\n",
        "sites = pd.read_csv('../data/sampling_sites_extended.csv')\n",
        "\n",
        "# use search engine to find demographic data\n",
        "search = SearchEngine() \n",
        "\n",
        "zipdata = [] \n",
        "\n",
        "for zipcode in sites['ZIP Code']: \n",
        "  info = search.by_zipcode(zipcode)\n",
        "\n",
        "  if bool(info) == True: \n",
        "    zipdata.append({\n",
        "      'ZIP Code': zipcode,\n",
        "      'housing_units': info.housing_units / 1000,\n",
        "      'occupied_housing_units': info.occupied_housing_units / 1000, \n",
        "      'median_home_value': info.median_home_value / 1000,\n",
        "      'median_household_income': info.median_household_income / 1000\n",
        "    })\n",
        "\n",
        "# add demographic data to sampling sites\n",
        "zipdata = pd.DataFrame(zipdata) \n",
        "sites = pd.merge(sites, zipdata, how = 'inner', on = 'ZIP Code')\n",
        "sites = sites.drop_duplicates()\n",
        "\n",
        "# merge with location based information\n",
        "sites.rename(columns = {'Sample Site': 'sample_site'}, inplace = True)\n",
        "\n",
        "data = pd.merge(data, sites, on = 'sample_site')"
      ],
      "id": "578ba65d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# set 'sample_date' as the index and sort by 'sample_date'\n",
        "data = data.set_index('sample_date')\n",
        "data = data.sort_values(by='sample_date')\n",
        "\n",
        "# prepare for modelling\n",
        "data = data.drop(columns = ['OBJECTID', 'sample_site', 'sample_class', \n",
        "                            'Sample Station (SS) - Location Description',\n",
        "                            'X - Coordinate', 'Y - Coordinate',\n",
        "                            'City or Placename'])\n",
        "\n",
        "# separate location data columns from the rest of the data\n",
        "location_columns = ['ZIP Code', 'latitude', 'longitude', 'housing_units', 'occupied_housing_units', \n",
        "                    'median_home_value', 'median_household_income']\n",
        "\n",
        "# identify the other columns (excluding location data)\n",
        "columns_to_scale = [col for col in data.columns if col not in location_columns]\n",
        "\n",
        "# scale only the non-location columns\n",
        "data[columns_to_scale] = (data[columns_to_scale] - data[columns_to_scale].mean()) / data[columns_to_scale].std()\n",
        "\n",
        "# save cleaned and merged data to new file\n",
        "data.to_csv('../data/data_merged.csv', index=False)"
      ],
      "id": "a892b9d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Borough information\n"
      ],
      "id": "23eb7801"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from uszipcode import SearchEngine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# lowercase column names\n",
        "data.columns = data.columns.str.lower()\n",
        "\n",
        "# initialize the search engine\n",
        "search = SearchEngine()\n",
        "\n",
        "# list of nyc boroughs\n",
        "nyc_boroughs = ['Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island']\n",
        "\n",
        "# function to get the borough only if it's one of the 5 boroughs\n",
        "def get_borough(zipcode):\n",
        "    info = search.by_zipcode(zipcode)\n",
        "    \n",
        "    if info and hasattr(info, 'major_city'):\n",
        "        # check if the major city corresponds to one of the 5 boroughs\n",
        "        major_city = info.major_city\n",
        "        if major_city in nyc_boroughs:\n",
        "            return major_city\n",
        "    return None  # return None if it's not one of the 5 boroughs\n",
        "\n",
        "# apply the function to 'zip code' column and create a new column 'borough'\n",
        "data['borough'] = data['zip code'].apply(lambda x: get_borough(str(x)))\n",
        "\n",
        "# fill missing boroughs with KNN\n",
        "\n",
        "# only use rows where borough is not null for training the model\n",
        "impute_X = data[data['borough'].notnull()][['latitude', 'longitude']]\n",
        "impute_Y = data[data['borough'].notnull()]['borough']\n",
        "\n",
        "# encode borough labels for classification\n",
        "encoder = LabelEncoder()\n",
        "impute_Y = encoder.fit_transform(impute_Y)\n",
        "\n",
        "# initialize KNN classifier and train it\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(impute_X, impute_Y)\n",
        "\n",
        "# for missing boroughs, use latitude and longitude to predict\n",
        "X_missing = data[data['borough'].isnull()][['latitude', 'longitude']]\n",
        "\n",
        "# predict missing boroughs\n",
        "predicted_labels = knn.predict(X_missing)\n",
        "predicted_boroughs = encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# fill missing boroughs in the data\n",
        "data.loc[data['borough'].isnull(), 'borough'] = predicted_boroughs\n",
        "\n",
        "# check for remaining missing borough values\n",
        "print(f'Missing boroughs after imputation: {data[\"borough\"].isnull().sum()}')\n",
        "\n",
        "# Save the updated data with borough names\n",
        "data.to_csv('../data/merged_data_with_borough.csv', index=False)"
      ],
      "id": "25cbb1f7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization\n"
      ],
      "id": "786b861d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "\n",
        "# load your data\n",
        "merged_df = pd.read_csv('../data/merged_data_with_borough.csv')\n",
        "\n",
        "# define the list of boroughs to plot\n",
        "boroughs_to_plot = ['Brooklyn', 'Bronx', 'Staten Island']\n",
        "\n",
        "# set up a figure to hold multiple subplots (3 boroughs)\n",
        "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10, 15))\n",
        "\n",
        "# loop over each borough and create a plot\n",
        "for i, borough in enumerate(boroughs_to_plot):\n",
        "    # filter data for the current borough\n",
        "    borough_data = merged_df[merged_df['borough'] == borough]\n",
        "    \n",
        "    # plotting the data for this borough\n",
        "    axes[i].plot(borough_data.index, borough_data['chlorine'], label='Chlorine (mg/L)', color='blue')\n",
        "    axes[i].plot(borough_data.index, borough_data['turbidity'], label='Turbidity (NTU)', color = 'orange')\n",
        "    axes[i].plot(borough_data.index, borough_data['coliform'], label='Coliform (MPN/100mL)', color = 'green')\n",
        "    axes[i].plot(borough_data.index, borough_data['ecoli'], label='E. coli (MPN/100mL)', color='red')\n",
        "    \n",
        "    # set labels and title for each subplot\n",
        "    axes[i].set_title(f'Water Quality Levels for {borough}', fontsize=14)\n",
        "    axes[i].set_xlabel('Sample Date', fontsize=12)\n",
        "    axes[i].set_ylabel('Level', fontsize=12)\n",
        "    axes[i].legend(loc='upper right')\n",
        "\n",
        "# adjust layout to avoid overlapping labels\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "a839a7e2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}