{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Project\"\n",
        "author: \"Suha Akach & Isabelle Perez\"\n",
        "toc: true\n",
        "number-sections: true\n",
        "highlight-style: pygments\n",
        "format: \n",
        "  html: \n",
        "    code-fold: true\n",
        "    html-math-method: katex\n",
        "  pdf: \n",
        "    geometry: \n",
        "      - top=30mm\n",
        "      - left=20mm\n",
        "  docx: default\n",
        "---\n",
        "\n",
        "\n",
        "# Import Data & Necessary Packages\n"
      ],
      "id": "23d13feb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
        "                                                            r2_score)\n",
        "from uszipcode import SearchEngine\n",
        "import seaborn as sns\n",
        "from pysal.lib import weights\n",
        "from pysal.model import spreg\n",
        "import gmplot\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "ad57682c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv('../data/Drinking_Water_Quality_Distribution_Monitoring_Data_20241116.csv',\n",
        "                                                                  low_memory = False)                       "
      ],
      "id": "b17ef20e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning \n"
      ],
      "id": "e1fc5460"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# rename columns\n",
        "data.rename(columns = {'Sample Date': 'sample_date', 'Sample Site': 'sample_site', \n",
        "              'Sample class': 'sample_class', 'Residual Free Chlorine (mg/L)': 'chlorine',\n",
        "              'Turbidity (NTU)': 'turbidity', 'Coliform (Quanti-Tray) (MPN /100mL)': 'coliform',\n",
        "                                      'E.coli(Quanti-Tray) (MPN/100mL)': 'ecoli'}, inplace = True)"
      ],
      "id": "4cb96952",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ensure sample time column is clean\n",
        "data.dropna(subset = ['Sample Time'], inplace = True)\n",
        "\n",
        "# standardize format of sample time\n",
        "data['Sample Time'] = data['Sample Time'].apply(lambda x: x[11:16] if len(x) > 5 else x)\n",
        "\n",
        "# change to datetime format\n",
        "data['sample_date'] = pd.to_datetime(data['sample_date'] + ' ' + data['Sample Time'])\n",
        "\n",
        "# change turbidity to float\n",
        "data.loc[data['turbidity'] == '<0.10', 'turbidity'] = '0.10'\n",
        "data['turbidity'] = data['turbidity'].apply(lambda x: float(x))"
      ],
      "id": "fb1100f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# drop unecessary columns and rows\n",
        "data.drop(columns = ['Sample Number', 'Fluoride (mg/L)', 'Sample Time'], inplace = True)"
      ],
      "id": "be65979b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# impute non-float values\n",
        "catval = {'<0.10': float(0.09), '<1': float(0.99), '>200.5': float(200.6)}\n",
        "data = data.replace(catval)\n",
        "\n",
        "# change data types to float\n",
        "data['chlorine'] = data['chlorine'].astype(float)\n",
        "data['coliform'] = data['coliform'].astype(float)\n",
        "data['ecoli'] = data['ecoli'].astype(float)"
      ],
      "id": "133d41e1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# check to make sure all values are possible\n",
        "data = data[data['chlorine'] > 0]\n",
        "data = data[data['coliform'] > 0]\n",
        "data = data[data['ecoli'] > 0]"
      ],
      "id": "afa3888e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# only 4 missing values total - drop\n",
        "data = data.dropna()\n",
        "\n",
        "data.shape"
      ],
      "id": "870045f3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in data for sampling sites\n",
        "sites = pd.read_csv('../data/sampling_sites_extended.csv')\n",
        "\n",
        "# use search engine to find demographic data\n",
        "search = SearchEngine() \n",
        "\n",
        "zipdata = [] \n",
        "\n",
        "for zipcode in sites['ZIP Code']: \n",
        "  info = search.by_zipcode(zipcode)\n",
        "\n",
        "  if bool(info) == True: \n",
        "    zipdata.append({\n",
        "      'ZIP Code': zipcode,\n",
        "      'housing_units': info.housing_units / 1000,\n",
        "      'occupied_housing_units': info.occupied_housing_units / 1000, \n",
        "      'median_home_value': info.median_home_value / 1000,\n",
        "      'median_household_income': info.median_household_income / 1000\n",
        "    })\n",
        "\n",
        "# add demographic data to sampling sites\n",
        "zipdata = pd.DataFrame(zipdata) \n",
        "sites = pd.merge(sites, zipdata, how = 'inner', on = 'ZIP Code')\n",
        "sites = sites.drop_duplicates()\n",
        "\n",
        "# merge with location based information\n",
        "sites.rename(columns = {'Sample Site': 'sample_site'}, inplace = True)\n",
        "\n",
        "data = pd.merge(data, sites, on = 'sample_site')"
      ],
      "id": "008a5816",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# sort data \n",
        "data = data.set_index('sample_date')\n",
        "data = data.sort_values(by = 'sample_date')\n",
        "\n",
        "# prepare for modelling\n",
        "data = data.drop(columns = ['OBJECTID', 'sample_class', \n",
        "                            'Sample Station (SS) - Location Description',\n",
        "                            'X - Coordinate', 'Y - Coordinate',\n",
        "                            'City or Placename'])\n",
        "\n",
        "# scale data\n",
        "data = (data - data.mean()) / data.std()"
      ],
      "id": "0fdf6991",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Borough information"
      ],
      "id": "e2cd12db"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# save cleaned and merged data to new file\n",
        "data.to_csv('../data/data_merged.csv', index=False)\n",
        "\n",
        "# lowercase column names\n",
        "data.columns = data.columns.str.lower()\n",
        "\n",
        "# use uszipcode to find borough from longitude, latitude, and zip code\n",
        "search = SearchEngine()\n",
        "\n",
        "# function to get borough from longitude, latitude, and zip code\n",
        "def get_borough(row):\n",
        "    try:\n",
        "        # Use ZIP Code and coordinates to get borough information\n",
        "        info = search.by_coordinates(row['latitude'], row['longitude'], radius=5)\n",
        "        if info:\n",
        "            return info[0].major_city  # This returns the 'borough' or major city name (if available)\n",
        "        else:\n",
        "            return None  # If no info is found, return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error retrieving borough for row {row['sample_site']}: {e}\")\n",
        "        return None\n",
        "\n",
        "# apply the function to each row of the data\n",
        "data['borough'] = data.apply(get_borough, axis=1)\n",
        "\n",
        "# save the final merged dataset to a new file\n",
        "data.to_csv('../data/data_merged_with_borough.csv', index=False)"
      ],
      "id": "a7c95d3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modelling\n"
      ],
      "id": "b69ca0ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def check_stationarity(series):\n",
        "    '''Check stationarity of column using Augmented Dickey-Fuller test'''\n",
        "    result = adfuller(series)\n",
        "    print(f'ADF Statistic: {result[0]}')\n",
        "    print(f'p-value: {result[1]}')\n",
        "    if result[1] <= 0.05:\n",
        "        print('The series is stationary.')\n",
        "        print()\n",
        "    else:\n",
        "        print('The series is not stationary.')\n",
        "        print()\n",
        "\n",
        "# check stationarities\n",
        "for col in ['chlorine', 'turbidity', 'coliform', 'ecoli']:\n",
        "    print(f'Checking stationarity for {col}')\n",
        "    check_stationarity(data[col])"
      ],
      "id": "c4450e46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# split into training and testing\n",
        "train_size = int(len(data) * 0.7)\n",
        "\n",
        "data_train = data[:train_size]\n",
        "data_test = data[train_size:]"
      ],
      "id": "070dcb11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = VAR(data_train)"
      ],
      "id": "33e070f8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lag_results = {}\n",
        "\n",
        "# iterate through range of possible lags\n",
        "for i in range(1, 31):\n",
        "  result = model.fit(i)\n",
        "  lag_results[i] = {\n",
        "    'AIC': result.aic,\n",
        "    'BIC': result.bic,\n",
        "    'HQIC': result.hqic,\n",
        "    }\n",
        "\n",
        "# find best lag by metric\n",
        "best_aic = min(lag_results, key = lambda x: lag_results[x]['AIC'])\n",
        "best_bic = min(lag_results, key = lambda x: lag_results[x]['BIC'])\n",
        "best_hqic = min(lag_results, key = lambda x: lag_results[x]['HQIC'])\n",
        "\n",
        "print(f'Best lag by AIC: {best_aic}')\n",
        "print(f'Best lag by BIC: {best_bic}')\n",
        "print(f'Best lag by HQIC: {best_hqic}')"
      ],
      "id": "54429ddf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lags = list(lag_results.keys())\n",
        "aic_values = [lag_results[lag]['AIC'] for lag in lags]\n",
        "bic_values = [lag_results[lag]['BIC'] for lag in lags]\n",
        "hqic_values = [lag_results[lag]['HQIC'] for lag in lags]\n",
        "\n",
        "# plot AIC, BIC, and HQIC for range of lags\n",
        "plt.plot(lags, aic_values, label='AIC', marker='o')\n",
        "plt.plot(lags, bic_values, label='BIC', marker='o')\n",
        "plt.plot(lags, hqic_values, label='HQIC', marker='o')\n",
        "plt.xlabel('Lag Order')\n",
        "plt.ylabel('Information Criterion')\n",
        "plt.title('Lag Order Selection (Custom Range)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "9f1f306b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# fit model\n",
        "results = model.fit(maxlags = 27)\n",
        "\n",
        "# create predictions\n",
        "forecast_input = data_train.values[-results.k_ar:]\n",
        "forecast = results.forecast(forecast_input, steps = len(data_test))\n",
        "forecast_df = pd.DataFrame(forecast, index = data_test.index, \n",
        "                                        columns = data_test.columns)"
      ],
      "id": "21b952f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "mae = mean_absolute_error(data_test, forecast_df)\n",
        "rmse = mean_squared_error(data_test, forecast_df, squared = False)\n",
        "\n",
        "print(f'MAE: {mae}')\n",
        "print(f'RMSE: {rmse}')"
      ],
      "id": "88d0675b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "for col in data_test.columns:\n",
        "  actual = data_test[col]\n",
        "  pred = forecast_df[col]\n",
        "\n",
        "  print(f'R-squared for {col} on test set: {r2_score(actual, pred):.4f}')"
      ],
      "id": "36f19fc4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "forecast_df\n",
        "#data_test\n",
        "\n",
        "\n",
        "#print(results.test_serial_correlation())\n",
        "print(results.summary())"
      ],
      "id": "a2e13542",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Suhak\\AppData\\Local\\Programs\\Python\\Python310\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}