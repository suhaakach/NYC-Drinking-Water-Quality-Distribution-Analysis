{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Final Project\"\n",
        "author: \"Suha Akach & Isabelle Perez\"\n",
        "toc: true\n",
        "number-sections: true\n",
        "highlight-style: pygments\n",
        "format: \n",
        "  html: \n",
        "    code-fold: true\n",
        "    html-math-method: katex\n",
        "  pdf: \n",
        "    geometry: \n",
        "      - top=30mm\n",
        "      - left=20mm\n",
        "  docx: default\n",
        "---\n",
        "\n",
        "# Objective Statement:\n",
        "\n",
        "The purpose of this project is to analyze factors affecting water quality as based off of national\n",
        "standards. Our dataset contains columns with measurements of various bacteria in water sampling\n",
        "sites across NYC. We will perform logistic regression, with the response variable being a binary\n",
        "column representing whether the sites are compliant with standards drinking water cleanliness.\n",
        "\n",
        "# Import Data & Necessary Packages\n"
      ],
      "id": "86631c92"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from sklearn.metrics import (mean_absolute_error, mean_squared_error,\n",
        "                                                            r2_score)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from uszipcode import SearchEngine\n",
        "import seaborn as sns\n",
        "from pysal.lib import weights\n",
        "from pysal.model import spreg\n",
        "import gmplot\n",
        "import warnings\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "743a0b1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = pd.read_csv('../data/Drinking_Water_Quality_Distribution_Monitoring_Data_20241116.csv',\n",
        "                                                                  low_memory = False)                       "
      ],
      "id": "be20911b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Cleaning \n"
      ],
      "id": "bf388c3c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# rename columns\n",
        "data.rename(columns = {'Sample Date': 'sample_date', 'Sample Site': 'sample_site', \n",
        "              'Sample class': 'sample_class', 'Residual Free Chlorine (mg/L)': 'chlorine',\n",
        "              'Turbidity (NTU)': 'turbidity', 'Coliform (Quanti-Tray) (MPN /100mL)': 'coliform',\n",
        "                                      'E.coli(Quanti-Tray) (MPN/100mL)': 'ecoli'}, inplace = True)"
      ],
      "id": "2c62cdc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fixing date columns\n",
        "The dates from the original dataset are written as strings and divided between two columns, one \n",
        "for the date and one for the time. We merged these two columns to create one column for the date\n",
        "and converted it to a datetime. "
      ],
      "id": "8bd824c0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ensure sample time column is clean\n",
        "data.dropna(subset = ['Sample Time'], inplace = True)\n",
        "\n",
        "# standardize format of sample time\n",
        "data['Sample Time'] = data['Sample Time'].apply(lambda x: x[11:16] if len(x) > 5 else x)\n",
        "\n",
        "# change to datetime format\n",
        "data['sample_date'] = pd.to_datetime(data['sample_date'] + ' ' + data['Sample Time'])\n",
        "\n",
        "# change turbidity to float\n",
        "data.loc[data['turbidity'] == '<0.10', 'turbidity'] = '0.10'\n",
        "data['turbidity'] = data['turbidity'].apply(lambda x: float(x))\n",
        "\n",
        "# Check the first few rows after creating 'sample_date'\n",
        "print(data[['sample_date']].head())"
      ],
      "id": "c8169b0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We dropped the following columns because they're unecessary to our investigation or have more than\n",
        "half the data missing.\n"
      ],
      "id": "c691d5cd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# drop unecessary columns and rows\n",
        "data.drop(columns = ['Sample Number', 'Fluoride (mg/L)', 'Sample Time'], inplace = True)"
      ],
      "id": "b3788e5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following code, we changed the categorical values for being less than/greater than to\n",
        "numerical values. Furthermore, we ensured that the columns for chlorine, coliform, ecoli, and\n",
        "turbidity were cast as float types."
      ],
      "id": "bebedb35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# impute non-float values\n",
        "catval = {'<0.10': float(0.09), '<1': float(0.99), '>200.5': float(200.6)}\n",
        "data = data.replace(catval)\n",
        "\n",
        "# change data types to float\n",
        "data['chlorine'] = data['chlorine'].astype(float)\n",
        "data['coliform'] = data['coliform'].astype(float)\n",
        "data['ecoli'] = data['ecoli'].astype(float)"
      ],
      "id": "26393527",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We made sure that there were no invalid values in the following columns based on possible values\n",
        "of ecoli, chlorine, coliform, and turbidity.\n"
      ],
      "id": "816f1a87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# check to make sure all values are possible\n",
        "data = data[data['chlorine'] > 0]\n",
        "data = data[data['coliform'] > 0]\n",
        "data = data[data['ecoli'] > 0]"
      ],
      "id": "34460eaa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There were only four remaining missing values, so I dropped the rest."
      ],
      "id": "db5b1890"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# only 4 missing values total - drop\n",
        "data = data.dropna()\n",
        "\n",
        "data.shape"
      ],
      "id": "63b941ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pulling Location Information from Site Sample Data\n",
        "\n",
        "Using the uszipcode package, we pulled demographic data for each sample site using the site's zip\n",
        "code, which was obtained from an additional dataset given along with the original data."
      ],
      "id": "622a7232"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# read in data for sampling sites\n",
        "sites = pd.read_csv('../data/sampling_sites_extended.csv')\n",
        "\n",
        "# use search engine to find demographic data\n",
        "search = SearchEngine() \n",
        "\n",
        "zipdata = [] \n",
        "\n",
        "for zipcode in sites['ZIP Code']: \n",
        "  info = search.by_zipcode(zipcode)\n",
        "\n",
        "  if bool(info) == True: \n",
        "    zipdata.append({\n",
        "      'ZIP Code': zipcode,\n",
        "      'housing_units': info.housing_units / 1000,\n",
        "      'occupied_housing_units': info.occupied_housing_units / 1000, \n",
        "      'median_home_value': info.median_home_value / 1000,\n",
        "      'median_household_income': info.median_household_income / 1000\n",
        "    })\n",
        "\n",
        "# add demographic data to sampling sites\n",
        "zipdata = pd.DataFrame(zipdata) \n",
        "sites = pd.merge(sites, zipdata, how = 'inner', on = 'ZIP Code')\n",
        "sites = sites.drop_duplicates()\n",
        "\n",
        "# merge with location based information\n",
        "sites.rename(columns = {'Sample Site': 'sample_site'}, inplace = True)\n",
        "\n",
        "data = pd.merge(data, sites, on = 'sample_site')"
      ],
      "id": "ea3606e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the original goal of this project had been to use a time series model, we sorted the data\n",
        "by sample date and dropped unecessary columns, such as the X and Y - coordinates, since Longitude\n",
        "and Latitude were already included.\n"
      ],
      "id": "0b3edb35"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "# set 'sample_date' as the index and sort by 'sample_date'\n",
        "data = data.set_index('sample_date')\n",
        "data = data.sort_values(by='sample_date')\n",
        "\n",
        "# prepare for modelling\n",
        "data = data.drop(columns = ['OBJECTID', 'sample_site', 'sample_class', \n",
        "                            'Sample Station (SS) - Location Description',\n",
        "                            'X - Coordinate', 'Y - Coordinate',\n",
        "                            'City or Placename'])\n",
        "\n",
        "# separate location data columns from the rest of the data\n",
        "location_columns = ['ZIP Code', 'latitude', 'longitude', 'housing_units', 'occupied_housing_units', \n",
        "                    'median_home_value', 'median_household_income']\n",
        "\n",
        "# identify the other columns (excluding location data)\n",
        "columns_to_scale = [col for col in data.columns if col not in location_columns]\n",
        "\n",
        "# scale only the non-location columns\n",
        "data[columns_to_scale] = (data[columns_to_scale] - data[columns_to_scale].mean()) / data[columns_to_scale].std()\n",
        "\n",
        "# save cleaned and merged data to new file\n",
        "data.to_csv('data/data_merged.csv', index=False)"
      ],
      "id": "4a019879",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Borough Information\n",
        "Using uszipcode again, we added a column for borough that was based off the zip code of the sample \n",
        "site. Furthermore, since some of the boroughs were still missing, these were imputed using \n",
        "KNeighborsClassifier and the latitude and longitude. If major cities were extracted instead, I created a mapping key to change them to their boroughs manually. This resulted in accurate information and we had no null rows!\n"
      ],
      "id": "83aac7ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| eval: false\n",
        "from uszipcode import SearchEngine\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# lowercase column names\n",
        "data.columns = data.columns.str.lower()\n",
        "\n",
        "# initialize the search engine\n",
        "search = SearchEngine()\n",
        "\n",
        "# list of nyc boroughs\n",
        "nyc_boroughs = ['Manhattan', 'Brooklyn', 'Queens', 'Bronx', 'Staten Island']\n",
        "\n",
        "# Define a dictionary mapping neighborhoods/cities to boroughs\n",
        "neighborhood_to_borough = {\n",
        "    # Manhattan neighborhoods\n",
        "    'New York': 'Manhattan',\n",
        "    'Harlem': 'Manhattan',\n",
        "    'Upper West Side': 'Manhattan',\n",
        "    'Upper East Side': 'Manhattan',\n",
        "    'Chelsea': 'Manhattan',\n",
        "    'SoHo': 'Manhattan',\n",
        "    'Greenwich Village': 'Manhattan',\n",
        "    'Tribeca': 'Manhattan',\n",
        "    'Financial District': 'Manhattan',\n",
        "    'Lower East Side': 'Manhattan',\n",
        "    'East Village': 'Manhattan',\n",
        "    'Midtown': 'Manhattan',\n",
        "\n",
        "    # Queens neighborhoods\n",
        "    'Forest Hills': 'Queens',\n",
        "    'Jackson Heights': 'Queens',\n",
        "    'Astoria': 'Queens',\n",
        "    'Ridgewood': 'Queens',\n",
        "    'Sunnyside': 'Queens',\n",
        "    'Flushing': 'Queens',\n",
        "    'Long Island City': 'Queens',\n",
        "    'Woodside': 'Queens',\n",
        "    'Bayside': 'Queens',\n",
        "    'Rego Park': 'Queens',\n",
        "    'Elmhurst': 'Queens',\n",
        "    'Kew Gardens': 'Queens',\n",
        "    'Oakland Gardens': 'Queens',\n",
        "    'Fresh Meadows': 'Queens',\n",
        "    'Whitestone': 'Queens',\n",
        "    'Corona': 'Queens',\n",
        "    'Maspeth': 'Queens',\n",
        "    'Jamaica': 'Queens',\n",
        "    'Queens Village': 'Queens',\n",
        "    'South Ozone Park': 'Queens',\n",
        "    'Far Rockaway': 'Queens',\n",
        "}\n",
        "\n",
        "def get_borough(zipcode):\n",
        "    # Fetch the information about the zipcode\n",
        "    info = search.by_zipcode(zipcode)\n",
        "    if info and info.major_city:\n",
        "        major_city = info.major_city\n",
        "        print(f\"Zipcode {zipcode}: major city = {major_city}\")  # Debugging output\n",
        "        \n",
        "        # First, check if the city or neighborhood is explicitly mapped to a borough\n",
        "        if major_city in neighborhood_to_borough:\n",
        "            return neighborhood_to_borough[major_city]\n",
        "        \n",
        "        # If not, check if it's one of the 5 NYC boroughs\n",
        "        if major_city in nyc_boroughs:\n",
        "            return major_city\n",
        "        \n",
        "    return None\n",
        "\n",
        "# Apply the function to the 'zip code' column to create a new 'borough' column\n",
        "data['borough'] = data['zip code'].apply(lambda x: get_borough(str(x)))\n",
        "\n",
        "# Handle missing boroughs using KNN\n",
        "\n",
        "# Only use rows where borough is not null for training the model\n",
        "impute_X = data[data['borough'].notnull()][['latitude', 'longitude']]\n",
        "impute_Y = data[data['borough'].notnull()]['borough']\n",
        "\n",
        "# Encode borough labels for classification\n",
        "encoder = LabelEncoder()\n",
        "impute_Y = encoder.fit_transform(impute_Y)\n",
        "\n",
        "# Initialize and train the KNN classifier\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(impute_X, impute_Y)\n",
        "\n",
        "# Predict missing boroughs based on latitude and longitude\n",
        "X_missing = data[data['borough'].isnull()][['latitude', 'longitude']]\n",
        "predicted_labels = knn.predict(X_missing)\n",
        "predicted_boroughs = encoder.inverse_transform(predicted_labels)\n",
        "\n",
        "# Fill missing boroughs in the data\n",
        "data.loc[data['borough'].isnull(), 'borough'] = predicted_boroughs\n",
        "\n",
        "# Check for remaining missing borough values\n",
        "print(f'Missing boroughs after imputation: {data[\"borough\"].isnull().sum()}')\n",
        "\n",
        "# manually update the boroughs based on the `neighborhood_to_borough` dictionary\n",
        "for neighborhood, borough in neighborhood_to_borough.items():\n",
        "    data.loc[data['borough'] == neighborhood, 'borough'] = borough\n",
        "\n",
        "# Save the updated DataFrame\n",
        "data.to_csv('data/merged_data_with_borough.csv', index=False)"
      ],
      "id": "a6caa3f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualization of Chemical Levels by the Least and Most Densely Populated Boroughs\n",
        "Made sure to apply the threshold levels that indicate maximum level of chemical presence."
      ],
      "id": "c1d09f77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "merged_df = pd.read_csv('../data/borough.csv')\n",
        "\n",
        "# list of boroughs to plot (only Manhattan and Staten Island)\n",
        "boroughs_to_plot = ['Staten Island', 'Manhattan']\n",
        "\n",
        "# figure to hold 4 subplots (one for each variable per borough)\n",
        "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15, 20))\n",
        "\n",
        "# threshold values for each chemical\n",
        "thresholds = {\n",
        "    'chlorine': 4,  # mg/L\n",
        "    'turbidity': 0.3,  # NTU\n",
        "    'coliform': 2.67,  # MPN/100mL\n",
        "    'ecoli': 0  # MPN/100mL\n",
        "}\n",
        "\n",
        "# loop over each borough and create plots\n",
        "for i, borough in enumerate(boroughs_to_plot):\n",
        "    # filter data for the current borough\n",
        "    borough_data = merged_df[merged_df['borough'] == borough]\n",
        "\n",
        "    # plot for Coliform levels (MPN/100mL)\n",
        "    ax = axes[0, i]  # Top row\n",
        "    ax.plot(borough_data.index, borough_data['coliform'], label='Coliform (MPN/100mL)', color='green')\n",
        "    ax.axhline(y=thresholds['coliform'], color='green', linestyle='--', label='Max Coliform Threshold (2.67 MPN/100mL)')\n",
        "    ax.set_title(f'Coliform Levels for {borough} (MPN/100mL)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for E. coli levels (MPN/100mL)\n",
        "    ax = axes[1, i]  # Second row\n",
        "    ax.plot(borough_data.index, borough_data['ecoli'], label='E. coli (MPN/100mL)', color='red')\n",
        "    ax.axhline(y=thresholds['ecoli'], color='red', linestyle='--', label='Max E. coli Threshold (126 MPN/100mL)')\n",
        "    ax.set_title(f'E. coli Levels for {borough} (MPN/100mL)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for Turbidity levels (NTU)\n",
        "    ax = axes[2, i]  # Third row\n",
        "    ax.plot(borough_data.index, borough_data['turbidity'], label='Turbidity (NTU)', color='orange')\n",
        "    ax.axhline(y=thresholds['turbidity'], color='orange', linestyle='--', label='Max Turbidity Threshold (0.3 NTU)')\n",
        "    ax.set_title(f'Turbidity Levels for {borough} (NTU)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "    # plot for Chlorine levels (mg/L)\n",
        "    ax = axes[3, i]  # Fourth row\n",
        "    ax.plot(borough_data.index, borough_data['chlorine'], label='Chlorine (mg/L)', color='blue')\n",
        "    ax.axhline(y=thresholds['chlorine'], color='blue', linestyle='--', label='Max Chlorine Threshold (4 mg/L)')\n",
        "    ax.set_title(f'Chlorine Levels for {borough} (mg/L)', fontsize=14)\n",
        "    ax.set_xlabel('Sample Date', fontsize=12)\n",
        "    ax.set_ylabel('Level', fontsize=12)\n",
        "    ax.legend(loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "id": "6fc250ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Logistic Regression Modelling \n",
        "\n",
        "## Analyzing how boroughs predict water quality\n",
        "### Function to preprocess and create the 'compliant' target variable\n",
        "\n",
        "We created a binary response variable, compliant, indicating whether or not the levels\n",
        "of chlorine, ecoli, turbidity, and coliform are all compliant with national drinking\n",
        "water standards. Ths was created as a binary variable, with $1$ indicating compliance and\n",
        "$0$ indicating noncompliance.\n"
      ],
      "id": "3838c3ca"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def preprocess_data(df):\n",
        "    df['compliant'] = (\n",
        "        (df['chlorine'] > 0.2) & (df['chlorine'] < 1.0) & \n",
        "        (df['ecoli'] <= 0) & \n",
        "        (df['turbidity'] < 0.3) & \n",
        "        (df['coliform'] < 2.67)\n",
        "    ).astype(int)\n",
        "    return df"
      ],
      "id": "62ffe769",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to resample the data using SMOTE\n",
        "We had way more instances of compliant water levels therfore we had to resample the minority class with synthetic data. "
      ],
      "id": "d79d001e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def resample_data(X, y):\n",
        "    smote = SMOTE(random_state=0)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled"
      ],
      "id": "6d2752e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function for hyperparameter tuning using GridSearchCV\n",
        "Tested to use both l1 and l2 regulization. This helped with penalitizng our features to focus on stronger predictors. We used grid search to find best hyperparameters which we then were able to use as final model. "
      ],
      "id": "524e6387"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def grid_search_tuning(X_train, y_train):\n",
        "    param_grid = {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10],\n",
        "        'penalty': ['elasticnet'],\n",
        "        'l1_ratio': [0.1, 0.5, 0.9],  # ratio between L1 and L2 (0 = L2, 1 = L1)\n",
        "        'solver': ['saga']\n",
        "    }\n",
        "\n",
        "    logreg = LogisticRegression(max_iter=1000)\n",
        "    grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='f1', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score (F1): {grid_search.best_score_}\")\n",
        "\n",
        "    return grid_search.best_estimator_"
      ],
      "id": "2c70b6ba",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to train and evaluate the model using the best parameters from GridSearchCV\n",
        "Set our threshold to 0.3 to help with class imbalance. We then fitted our Logistic Regression model using our best parameters. This helped us optimize our model tremendously. "
      ],
      "id": "5aa20fd4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def train_and_evaluate_model(X_train, y_train, X_test, y_test, best_logreg, threshold=0.3):\n",
        "    # standardize the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # fit the Logistic Regression model with the best parameters\n",
        "    best_logreg.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # get predicted probabilities and adjust threshold\n",
        "    y_pred_prob = best_logreg.predict_proba(X_test_scaled)[:, 1]\n",
        "    y_pred_adjusted = (y_pred_prob > threshold).astype(int)\n",
        "\n",
        "    # evaluation metrics\n",
        "    print('Accuracy:', accuracy_score(y_test, y_pred_adjusted))\n",
        "    print('Precision:', precision_score(y_test, y_pred_adjusted))\n",
        "    print('Recall:', recall_score(y_test, y_pred_adjusted))\n",
        "    print('F1 Score:', f1_score(y_test, y_pred_adjusted))\n",
        "\n",
        "    cm = confusion_matrix(y_test, y_pred_adjusted)\n",
        "    print(cm)\n",
        "\n",
        "    return best_logreg"
      ],
      "id": "a973ad71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to plot feature importance\n",
        "Our L1 and L2 regulization helped us pentalize the least significant coefficients to zero therefore we got more accurate results. This showed us that location features slighltly help with predicting water quality. "
      ],
      "id": "c2d0cae2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_feature_importance(model, X):\n",
        "    coefficients = model.coef_[0]\n",
        "    feature_names = X.columns\n",
        "\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_names,\n",
        "        'Coefficient': coefficients,\n",
        "        'Absolute Coefficient': abs(coefficients)\n",
        "    })\n",
        "    \n",
        "    feature_importance_df = feature_importance_df.sort_values(by='Absolute Coefficient', ascending=False)\n",
        "\n",
        "    # feature importance\n",
        "    print(feature_importance_df)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(feature_importance_df['Feature'], feature_importance_df['Absolute Coefficient'], color='skyblue')\n",
        "    plt.xlabel('Absolute Coefficient')\n",
        "    plt.title('Feature Importance (Logistic Regression)')\n",
        "    plt.gca().invert_yaxis()  # Invert y-axis to show the most important features at the top\n",
        "    plt.show()"
      ],
      "id": "37d7aad4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function to plot the learning curve\n",
        "We wanted to see how our model is training and if its improvng our F1 score. We see from the curve that it is giving us a stable and high f1 score as the models fits more trainintgs."
      ],
      "id": "639d0792"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_learning_curve(model, X_train, y_train):\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        model, X_train, y_train, cv=5, n_jobs=-1,\n",
        "        train_sizes=np.linspace(0.1, 1.0, 10), scoring='f1')\n",
        "\n",
        "    # calculate the mean and standard deviation for plotting\n",
        "    train_mean = train_scores.mean(axis=1)\n",
        "    train_std = train_scores.std(axis=1)\n",
        "    test_mean = test_scores.mean(axis=1)\n",
        "    test_std = test_scores.std(axis=1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_sizes, train_mean, color='blue', label='Training score')\n",
        "    plt.plot(train_sizes, test_mean, color='red', label='Cross-validation score')\n",
        "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
        "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='red')\n",
        "    plt.title(\"Learning Curve (Logistic Regression)\")\n",
        "    plt.xlabel(\"Number of Training Samples\")\n",
        "    plt.ylabel(\"F1 Score\")\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.show()"
      ],
      "id": "b59096d4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main pipeline\n",
        "Overall methods and variables used."
      ],
      "id": "33ff8779"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# step 1: preprocess data\n",
        "merged_df = preprocess_data(merged_df)\n",
        "\n",
        "# step 2: get dummy variables\n",
        "model_data = pd.get_dummies(merged_df, dtype=int)\n",
        "model_data.drop(columns=['zip code'], inplace=True)\n",
        "\n",
        "X = model_data.drop(columns=['compliant'])\n",
        "y = model_data['compliant']\n",
        "\n",
        "# step 3: resample data\n",
        "X_resampled, y_resampled = resample_data(X, y)\n",
        "\n",
        "# step 4: split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=4188)\n",
        "\n",
        "# step 5: perform gridsearchcv to find the best hyperparameters\n",
        "best_logreg = grid_search_tuning(X_train, y_train)\n",
        "\n",
        "# step 6: train and evaluate the logistic regression model with the best hyperparameters\n",
        "best_logreg = train_and_evaluate_model(X_train, y_train, X_test, y_test, best_logreg)\n",
        "\n",
        "# step 7: plot the learning curve\n",
        "plot_learning_curve(best_logreg, X_train, y_train)\n",
        "\n",
        "# step 8: plot feature importance\n",
        "plot_feature_importance(best_logreg, X)"
      ],
      "id": "71bf8cf4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Analysis & Overall Results:\n",
        "\n",
        "The goal of this project was to analyze factors that affect water quality and predict whether water sampling sites across NYC comply with national standards for drinking water cleanliness. The dataset contains measurements of various bacteria in water samples, and we used logistic regression to predict compliance, with the response variable being a binary classification: compliant (1) or non-compliant (0).\n",
        "\n",
        "We began by importing the required libraries and packages necessary for data analysis, including pandas, scikit-learn, imblearn, and visualization libraries like matplotlib.\n",
        "\n",
        "In terms of data cleaning, we started with fixing date columns. We merged separate date and time columns into a single datetime column for easier analysis. We dropped unnecessary columns. We removed columns with irrelevant information or missing data, ensuring the dataset was clean and ready for analysis.\n",
        "\n",
        "Data Type Conversions: We converted categorical values (less than/greater than) to numerical values and ensured key columns (e.g., chlorine, ecoli, coliform) were cast as float types.\n",
        "\n",
        "Handling Missing Data: After addressing missing values, we ended up with a cleaned dataset of size (148701, 7).\n",
        "\n",
        "Pulling Location Information:\n",
        "Using the uszipcode package, we pulled demographic data based on the zip code of each sample site. We added borough information and imputed missing borough data using K-Nearest Neighbors and geographic coordinates (latitude and longitude).\n",
        "\n",
        "Visualization of Chemical Levels by Borough:\n",
        "We visualized the chemical levels for the least and most densely populated boroughs. We applied the relevant threshold values for chemical concentrations to observe trends in different locations.\n",
        "\n",
        "Logistic Regression Modelling:\n",
        "We defined a binary target variable compliant based on water quality standards for chlorine, ecoli, turbidity, and coliform. A value of 1 indicates compliance, and 0 indicates non-compliance. Since the dataset was imbalanced (more compliant than non-compliant samples), we used SMOTE (Synthetic Minority Over-sampling Technique) to generate synthetic data for the minority class (non-compliant) to improve model performance. We performed hyperparameter tuning to find the best logistic regression parameters using grid search. We tested different values for C (regularization strength), penalty, l1_ratio (for elastic net regularization), and the solver. The best parameters found were:\n",
        "\n",
        "C: 0.001\n",
        "Penalty: elasticnet\n",
        "L1 Ratio: 0.9\n",
        "Solver: saga\n",
        "\n",
        "We used the best hyperparameters from the grid search and trained the logistic regression model on the data. We set a threshold of 0.3 for classifying non-compliant water. This adjustment helped to deal with the class imbalance and optimized model performance. We analyzed the importance of features in predicting water quality. Regularization (L1 and L2) helped penalize less significant features. The most important features influencing predictions were chlorine levels, turbidity, and the geographical location of the sampling sites (boroughs). We plotted the learning curve to monitor model performance over increasing training sizes. The curve showed stable and high F1 scores, indicating that the model improved with more data. The pipeline incorporated all the steps, from preprocessing data to model evaluation:\n",
        "\n",
        "Best parameters: {'C': 0.001, 'l1_ratio': 0.9, 'penalty': 'elasticnet', 'solver': 'saga'}\n",
        "\n",
        "Best cross-validation score (F1): 0.808\n",
        "\n",
        "Model Evaluation:\n",
        "\n",
        "Accuracy: 74.92%\n",
        "Precision: 66.64%\n",
        "Recall: 99.71%\n",
        "F1 Score: 79.89%\n",
        "Confusion Matrix:\n",
        "\n",
        "[[15390 15297]\n",
        " [   88 30562]]\n",
        "\n",
        "The model showed good performance, especially with recall, as it detected almost all compliant water samples (high recall). However, the precision was slightly lower, meaning it sometimes misclassified non-compliant water as compliant.\n",
        "\n",
        "The features with the highest absolute coefficients (indicating their importance) were:\n",
        "\n",
        "Chlorine: 1.036\n",
        "Turbidity: 1.017\n",
        "borough_Manhattan: 0.132902\n",
        "borough_Brooklyn: 0.095154\n",
        "median_home_value: 0.076015\n",
        "median_household_income: 0.073837\n",
        "\n",
        "Borough information (Manhattan, Brooklyn) contributed to the prediction, though less significantly.\n",
        "The coefficients for ecoli, housing_units, and some other features were near zero, suggesting they had little impact on the prediction.\n",
        "\n",
        "Final Statements:\n",
        "- Chlorine and turbidity were the most significant predictors of water quality.\n",
        "- Boroughs (Manhattan and Brooklyn) had a moderate impact on water quality predictions.\n",
        "- The model achieved a high recall, making it effective at detecting compliant water, but we can still improve precision by reducing false positives.\n",
        "\n",
        "In conclusion, our logistic regression model is moderatly good for predicting water quality compliance based on environmental and socio-economic factors in NYC. "
      ],
      "id": "543e1097"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\Suhak\\AppData\\Local\\Programs\\Python\\Python310\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}